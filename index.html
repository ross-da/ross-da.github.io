<!DOCTYPE HTML>
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>David Ross</title>

  <meta name="author" content="David Ross">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="images/DR_icon.png">
</head>

<body>
  <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:65%;vertical-align:middle">
              <p style="text-align:center">
                <name>David Ross</name>
                <br>
                 <a href="mailto:daross@gmail.com" style="color:grey">daross at gmail dot com</a>
                <br>
                <br>
              </p>
              <p>I co-lead the VIVID research group, at
    <a href="https://deepmind.google/">Google DeepMind</a>. Our goal
    is to advance video understanding & generation, and amplify human
    capabilities with AI.
              </p>
              <p>
                Previously I led the YouTube Mix team that built the personalized algorithmic radio feature at the heart of <a href="https://music.youtube.com/">YouTube Music</a>.
              </p>
              <p>
                I obtained my Ph.D. in Machine Learning and Computer Vision from the University of Toronto, Canada.
              </p>
              <p style="text-align:center">
                <a href="http://scholar.google.com/citations?user=RqOzJR0AAAAJ">Google Scholar</a>
                 | 
                <a href="https://www.linkedin.com/in/david-ross-5693954/">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:40%;max-width:40%">
              <a href="images/daross_profile.jpg"><img style="width:100%;max-width:100%" alt="profile photo" src="images/daross_profile.jpg" class="hoverZoomLink"></a>
            </td>
          </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <heading>Current Work</heading>
                <p>
                    Our work on <a href="https://research.google/blog/videopoet-a-large-language-model-for-zero-shot-video-generation/">VideoPoet, a large language model for zero-shot video generation</a>, won Best Paper at ICML 2024. See the great overview by <a href="https://www.youtube.com/watch?v=zCNrB4wNJUU">Two Minute Papers</a>, the <a href="https://research.google/blog/videopoet-a-large-language-model-for-zero-shot-video-generation/">Google Research blog post</a>, and the <a href="https://sites.research.google/videopoet/">VideoPoet website</a>.<br><br>
                    Some of the latest open source releases from my team: <a href="https://blog.tensorflow.org/2020/07/tensorflow-2-meets-object-detection-api.html">TF Object Detection API for TensorFlow 2.x</a>, <a href="https://ai.googleblog.com/2021/02/3d-scene-understanding-with-tensorflow.html">TF3D for 3D Scene Understanding</a>, and the <a href="https://google.github.io/aistplusplus_dataset/">AIST++ Human Motion dataset</a>.
                    <br>
                    The results of the 3rd AVA Action Detection challenge are <a href="https://research.google.com/ava/challenge.html">now available</a>. This event was held at CVPR 2020, in partnership with the <a href="http://activity-net.org/challenges/2020/index.html">International Challenge on Activity Recognition (ActivityNet)</a> workshop.
                    <br>
                    My talk <a href="https://www.youtube.com/watch?v=IRYKpz6txeY&t=29703">Context & Attention for Detecting Objects and Actions in Video</a> at the CVPR'20 <a href="https://holistic-video-understanding.github.io/tutorials/cvpr2020.html">LSHVU Tutorial</a> is available on YouTube.
                    <br>
                    Our work on <a href="https://ai.googleblog.com/2019/04/capturing-special-video-moments-with.html">Capturing Special Video Moments with Google Photos</a> was just featured on the Google AI Blog.
                </p>
              </td>
            </tr>
        </tbody></table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                    <heading>Publications</heading>
                    <p>
                        A complete list of my publications and patents at <a href="http://scholar.google.com/citations?user=RqOzJR0AAAAJ">Google Scholar Citations</a>.
                    </p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2502.12632"><papertitle>MALT Diffusion: Memory-Augmented Latent Transformers for Any-Length Video Generation.</papertitle></a>
                  <br>
                  Sihyun Yu, Meera Hahn, Dan Kondratyuk, Jinwoo Shin, Agrim Gupta, José Lezama, Irfan Essa, David Ross, and Jonathan Huang
                  <br>
                  <em>CVPR Workshop on AI for Content Creation</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2502.12632">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2412.05796"><papertitle>Language-Guided Image Tokenization for Generation.</papertitle></a>
                  <br>
                  Kaiwen Zha, Lijun Yu, Alireza Fathi, David A. Ross, Cordelia Schmid, Dina Katabi, and Xiuye Gu
                  <br>
                  <em>The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 2025
                  <br>
                  <a href="https://arxiv.org/abs/2412.05796">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2312.14125"><papertitle>VideoPoet: A large language model for zero-shot video generation.</papertitle></a>
                  <br>
                  Dan Kondratyuk, Lijun Yu, Xiuye Gu, José Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Josh Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, Ming-Hsuan Yang, Irfan Essa, Huisheng Wang, David A. Ross, Bryan Seybold, and Lu Jiang
                  <br>
                  <em>Proceedings of International Conference on Machine Learning (ICML)</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2312.14125">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2402.13217"><papertitle>Videoprism: A foundational visual encoder for video understanding.</papertitle></a>
                  <br>
                  Long Zhao, Nitesh B. Gundavarapu, Liangzhe Yuan, Hao Zhou, Shen Yan, Jennifer J. Sun, Luke Friedman, Rui Qian, Tobias Weyand, Yue Zhao, and others
                  <br>
                  <em>Proceedings of International Conference on Machine Learning (ICML)</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2402.13217">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <papertitle>Scenecraft: An llm agent for synthesizing 3d scenes as blender code.</papertitle>
                  <br>
                  Ziniu Hu, Ahmet Iscen, Aashi Jain, Thomas Kipf, Yisong Yue, David A. Ross, Cordelia Schmid, and Alireza Fathi
                  <br>
                  <em>Proceedings of International Conference on Machine Learning (ICML)</em>, 2024
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <papertitle>Video Foundation Models for Animal Behavior Analysis.</papertitle>
                  <br>
                  Jennifer J. Sun, Hao Zhou, Long Zhao, Liangzhe Yuan, Bryan Seybold, David Hendon, Florian Schroff, David A. Ross, Hartwig Adam, Bo Hu, and others
                  <br>
                  <em>bioRxiv</em>, 2024
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2212.05221"><papertitle>Reveal: Retrieval-augmented visual-language pre-training with multi-source multimodal knowledge memory.</papertitle></a>
                  <br>
                  Ziniu Hu, Ahmet Iscen, Chen Sun, Zirui Wang, Kai-Wei Chang, Yizhou Sun, Cordelia Schmid, David A. Ross, and Alireza Fathi
                  <br>
                  <em>Proceedings of the IEEE/CVF conference on computer vision and pattern recognition</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2212.05221">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2302.01328"><papertitle>IC3: Image Captioning by Committee Consensus.</papertitle></a>
                  <br>
                  David M Chan, Austin Myers, Sudheendra Vijayanarasimhan, David A. Ross, and John Canny
                  <br>
                  <em>Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2302.01328">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://openreview.net/forum?id=kXOXrVnwbb"><papertitle>Dataseg: Taming a universal multi-dataset multi-task segmentation model.</papertitle></a>
                  <br>
                  Xiuye Gu, Yin Cui, Jonathan Huang, Abdullah Rashwan, Xuan Yang, Xingyi Zhou, Golnaz Ghiasi, Weicheng Kuo, Huizhong Chen, Liang-Chieh Chen, and David A. Ross
                  <br>
                  <em>Advances in Neural Information Processing Systems</em>, 2023
                  <br>
                  <a href="https://openreview.net/forum?id=kXOXrVnwbb">OpenReview</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2306.08129"><papertitle>Avis: Autonomous visual information seeking with large language model agent.</papertitle></a>
                  <br>
                  Ziniu Hu, Ahmet Iscen, Chen Sun, Kai-Wei Chang, Yizhou Sun, David Ross, Cordelia Schmid, and Alireza Fathi
                  <br>
                  <em>Advances in Neural Information Processing Systems</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2306.08129">arXiv</a> / <a href="https://research.google/blog/autonomous-visual-information-seeking-with-large-language-models/">Google AI blog</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <papertitle>3D mouse pose from single-view video and a new dataset.</papertitle>
                  <br>
                  Bo Hu, Bryan Seybold, Shan Yang, Avneesh Sud, Yi Liu, Karla Barron, Paulyn Cha, Marcelo Cosino, Ellie Karlsson, Janessa Kite, Ganesh Kolumam, Joseph Preciado, José Zavala-Solorio, Chunlian Zhang, Xiaomeng Zhang, Martin Voorbach, Ann E. Tovcimak, J. Graham Ruby, and David A. Ross
                  <br>
                  <em>Scientific Reports</em>, 2023
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2212.10596"><papertitle>Open-vocabulary temporal action detection with off-the-shelf image-text features.</papertitle></a>
                  <br>
                  Vivek Rathod, Bryan Seybold, Sudheendra Vijayanarasimhan, Austin Myers, Xiuye Gu, Vighnesh Birodkar, and David A. Ross
                  <br>
                  <em>arXiv preprint arXiv:2212.10596</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2212.10596">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2310.05737"><papertitle>Language Model Beats Diffusion -- Tokenizer is Key to Visual Generation.</papertitle></a>
                  <br>
                  Lijun Yu. José Lezama. Nitesh Bharadwaj Gundavarapu. Luca Versari. Kihyuk Sohn. David Minnen. Yong Cheng. Agrim Gupta. Xiuye Gu. Alex Hauptmann. Boqing Gong. Ming-Hsuan Yang. Irfan Essa. David Ross. Lu Jiang.
                  <br>
                  <em>ICLR</em>, 2024
                  <br>
                  <a href="https://arxiv.org/abs/2310.05737">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2308.11062"><papertitle>UnLoc: a unified framework for video localization tasks.</papertitle></a>
                  <br>
                  Shen Yan. Xuehan Xiong. Arsha Nagrani. Anurag Arnab. Zhonghao Wang. Weina Ge. David Ross. Cordelia Schmid.
                  <br>
                  <em>International Conference on Computer Vision (ICCV)</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2308.11062">arXiv</a> / <a href="https://github.com/google-research/scenic">open source implementation</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2306.17842"><papertitle>SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs.</papertitle></a>
                  <br>
                  Lijun Yu. Yong Cheng. Zhiruo Wang. Vivek Kumar. Wolfgang Macherey. Yanping Huang. David Ross. Irfan Essa. Yonatan Bisk. Ming-Hsuan Yang. Kevin Murphy. Alex Hauptmann. Lu Jiang.
                  <br>
                  <em>NeurIPS</em>, 2023
                  <br>
                  <a href="https://arxiv.org/abs/2306.17842">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2209.07518"><papertitle>Distribution Aware Metrics for Conditional Natural Language Generation.</papertitle></a>
                  <br>
                  David M Chan, Yiming Ni, Austin Myers, Sudheendra Vijayanarasimhan, David A Ross, and John Canny
                  <br>
                  <em>arXiv preprint</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2209.07518">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2209.04061"><papertitle>im2nerf: Image to Neural Radiance Field in the Wild.</papertitle></a>
                  <br>
                  Lu Mi, Abhijit Kundu, David Ross, Frank Dellaert, Noah Snavely, and Alireza Fathi
                  <br>
                  <em>arXiv preprint</em>, 2022
                  <br>
                  <a href="https://arxiv.org/abs/2209.04061">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2205.06253"><papertitle>What’s in a Caption? Dataset-Specific Linguistic Diversity and Its Effect on Visual Description Models and Metrics.</papertitle></a>
                  <br>
                  David M. Chan, Austin Myers, Sudheendra Vijayanarasimhan, David A. Ross, Bryan Seybold, John F. Canny
                  <br>
                  <em><a href="https://sites.google.com/view/vdu-cvpr22">The 1st Workshop on Vision Datasets Understanding, at CVPR 2022</a></em>
                  <br>
                  <a href="https://arxiv.org/abs/2205.06253">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2106.09251"><papertitle>Optical Mouse: 3D Mouse Pose From Single-View Video.</papertitle></a>
                  <br>
                  Bo Hu, Bryan Seybold, Shan Yang, David Ross, Avneesh Sud, Graham Ruby, and Yi Liu
                  <br>
                  <em><a href="https://www.cv4animals.com/paper">CV4Animals: Computer Vision for Animal Behavior Tracking and Modeling Workshop, at CVPR 2021</a></em>
                  <br>
                  <a href="https://arxiv.org/abs/2106.09251">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://google.github.io/aichoreographer/"><papertitle>AI Choreographer Music Conditioned 3D Dance Generation with AIST++.</papertitle></a>
                  <br>
                  Ruilong Li, Shan Yang, David A. Ross, Angjoo Kanazawa
                  <br>
                  <em>ICCV</em>, 2021
                  <br>
                  <a href="https://arxiv.org/abs/2101.08779">arXiv</a> / <a href="https://google.github.io/aichoreographer/">project website, dataset</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2007.14937"><papertitle>Learning Video Representations from Textual Web Supervision.</papertitle></a>
                  <br>
                  Jonathan C. Stroud, David A. Ross, Chen Sun, Jia Deng, Rahul Sukthankar, Cordelia Schmid
                  <br>
                  <em>arXiv</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2007.14937">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/Chan_Active_Learning_for_Video_Description_With_Cluster-Regularized_Ensemble_Ranking_ACCV_2020_paper.pdf"><papertitle>Active Learning for Video Description With Cluster-Regularized Ensemble Ranking.</papertitle></a>
                  <br>
                  David Chan, Sudheendra Vijayanarasimhan, David Ross, John Canny
                  <br>
                  <em>ACCV</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2007.13913">arXiv</a> / <a href="https://www.cs.toronto.edu/~dross/Chan_Active_Learning_for_Video_Description_With_Cluster-Regularized_Ensemble_Ranking_ACCV_2020_paper.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/Chan_Active_Learning_for_ACCV_2020_supplemental.pdf">supplementary</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2007.12392"><papertitle>An LSTM Approach to Temporal 3D Object Detection in LiDAR Point Clouds.</papertitle></a>
                  <br>
                  Rui Huang, Wanyue Zhang, Tom Funkhouser, Abhijit Kundu, David Ross, Caroline Pantofaru, Alireza Fathi
                  <br>
                  <em>ECCV</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2007.12392">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2007.10323"><papertitle>Pillar-based Object Detection for Autonomous Driving.</papertitle></a>
                  <br>
                  Yue Wang, Abhijit Kundu, Alireza Fathi, Caroline Pantofaru, David Ross, Justin Solomon, Tom Funkhouser
                  <br>
                  <em>ECCV</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2007.10323">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2007.13138"><papertitle>Virtual Multi-view Fusion for 3D Semantic Segmentation.</papertitle></a>
                  <br>
                  Abhijit Kundu, Xiaoqi (Michael) Yin, Alireza Fathi, Brew Barrington, David Ross, Tom Funkhouser, Caroline Pantofaru
                  <br>
                  <em>ECCV</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2007.13138">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://research.google.com/ava"><papertitle>The AVA-Kinetics Localized Human Actions Video Dataset.</papertitle></a>
                  <br>
                  Ang Li, Meghana Thotakuri, David A. Ross, João Carreira, Alexander Vostrikov, Andrew Zisserman
                  <br>
                  <em>arXiv</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2005.00214">arXiv</a> / <a href="https://research.google.com/ava">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/2004.01170"><papertitle>DOPS: Learning to Detect 3D Objects and Predict their 3D Shapes.</papertitle></a>
                  <br>
                  Mahyar Najibi, Guangda Lai, Abhijit Kundu, Zhichao Lu, Vivek Rathod, Thomas Funkhouser, Caroline Pantofaru, David Ross, Larry S. Davis, Alireza Fathi
                  <br>
                  <em>CVPR</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/2004.01170">arXiv</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/nagrani20.pdf"><papertitle>Speech2Action: Cross-modal Supervision for Action Recognition.</papertitle></a>
                  <br>
                  Arsha Nagrani, Chen Sun, David Ross, Rahul Sukthankar, Cordelia Schmid, Andrew Zisserman
                  <br>
                  <em>CVPR</em>, 2020
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/nagrani20.pdf">PDF</a> / <a href="https://arxiv.org/abs/2003.13594">arXiv</a> / <a href="https://www.robots.ox.ac.uk/~vgg/research/speech2action/">project page, data</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/1812.08249"><papertitle>D3D: Distilled 3D Networks for Video Action Recognition.</papertitle></a>
                  <br>
                  Jonathan C. Stroud, David A. Ross, Chen Sun, Jia Deng, Rahul Sukthankar
                  <br>
                  <em>WACV</em>, 2020
                  <br>
                  <a href="https://arxiv.org/abs/1812.08249">arXiv</a> / <a href="https://github.com/princeton-vl/d3dhelper">code and pre-trained models</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://arxiv.org/abs/1804.07667"><papertitle>Rethinking the Faster R-CNN Architecture for Temporal Action Localization.</papertitle></a>
                  <br>
                  Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia Deng, Rahul Sukthankar
                  <br>
                  <em>CVPR</em>, 2018
                  <br>
                  <a href="https://arxiv.org/abs/1804.07667">arXiv</a> / <a href="https://ai.googleblog.com/2019/04/capturing-special-video-moments-with.html">Google AI blog</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://research.google.com/ava"><papertitle>AVA: A Video Dataset of Spatio-temporally Localized Atomic Visual Actions.</papertitle></a>
                  <br>
                  Chunhui Gu, Chen Sun, David A. Ross, Carl Vondrick, Caroline Pantofaru, Yeqing Li, Sudheendra Vijayanarasimhan, George Toderici, Susanna Ricco, Rahul Sukthankar, Cordelia Schmid, Jitendra Malik
                  <br>
                  <em>CVPR</em>, 2018
                  <br>
                  <a href="https://arxiv.org/abs/1705.08421">arXiv</a> / <a href="https://research.google.com/ava">project website</a> / <a href="https://ai.googleblog.com/2017/10/announcing-ava-finely-labeled-video.html">Google AI blog</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/MadaniGeorgRoss2013.pdf"><papertitle>On using nearly-independent feature families for high precision and confidence.</papertitle></a>
                  <br>
                  Omid Madani, Manfred Georg, David Ross
                  <br>
                  <em>Machine Learning Journal</em>, 2013
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/MadaniGeorgRoss2013.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/WaltersRossLyon2012.pdf"><papertitle>The Intervalgram: An audio feature for large-scale melody recognition.</papertitle></a>
                  <br>
                  Thomas C. Walters, David Ross, Richard F. Lyon
                  <br>
                  <em>9th International Symposium on Computer Music Modeling and Retrieval (CMMR 2012)</em>
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/WaltersRossLyon2012.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/MadaniGeorgRoss2012.pdf"><papertitle>On Using Nearly-Independent Feature Families for High Precision and Confidence.</papertitle></a>
                  <br>
                  Omid Madani, Manfred Georg, David Ross
                  <br>
                  <em>4th Asian Conference on Machine Learning (ACML 2012)</em>
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/MadaniGeorgRoss2012.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/ChandrasekharSharifiRoss_ISMIR2011.pdf"><papertitle>Survey and Evaluation of Audio Fingerprinting Schemes for Mobile Query-by-Example Applications.</papertitle></a>
                  <br>
                  Vijay Chandrasekhar, Matt Sharifi, David Ross
                  <br>
                  <em>12th International Society for Music Information Retrieval Conference (ISMIR 2011)</em>
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/ChandrasekharSharifiRoss_ISMIR2011.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/YagnikStrelowRossLin_ICCV2011.pdf"><papertitle>The Power of Comparative Reasoning.</papertitle></a>
                  <br>
                  <a href="http://research.google.com/pubs/author36197.html">Jay Yagnik</a>, Dennis Strelow, David Ross, Ruei-Sung Lin
                  <br>
                  <em>ICCV</em>, 2011
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/YagnikStrelowRossLin_ICCV2011.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/ChandrasekharSarginRoss_ICASSP2011.pdf"><papertitle>Automatic Language Identification in Music Videos with Low Level Audio and Visual Features.</papertitle></a>
                  <br>
                  Vijay Chandrasekhar, Mehmet Emre Sargin, and David Ross
                  <br>
                  <em>ICASSP</em>, 2011
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/ChandrasekharSarginRoss_ICASSP2011.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/LinRossYagnik_CVPR2010.pdf"><papertitle>SPEC Hashing: Similarity Preserving algorithm for Entropy-based Coding.</papertitle></a>
                  <br>
                  Ruei-Sung Lin, David Ross, and <a href="http://research.google.com/pubs/author36197.html">Jay Yagnik</a>
                  <br>
                  <em>CVPR</em>, 2010
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/LinRossYagnik_CVPR2010.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/articulated/RossTarlowZemel_IJCV2010.pdf"><papertitle>Learning Articulated Structure and Motion.</papertitle></a>
                  <br>
                  David Ross, <a href="http://www.cs.toronto.edu/~dtarlow/">Daniel Tarlow</a>, and <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>
                  <br>
                  <em>International Journal of Computer Vision, 88 (2)</em>, 2010
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/articulated/RossTarlowZemel_IJCV2010.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/articulated/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/phd/Ross_David_A_200811_PhD_thesis.pdf"><papertitle>Learning Probabilistic Models for Visual Motion.</papertitle></a>
                  <br>
                  David Ross
                  <br>
                  <em>Ph.D. Thesis, University of Toronto, Canada</em>, 2008
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/phd/Ross_David_A_200811_PhD_thesis.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/phd/">videos</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/articulated/RossTarlowZemel_ECCV2008.pdf"><papertitle>Unsupervised learning of skeletons from motion.</papertitle></a>
                  <br>
                  David Ross, <a href="http://www.cs.toronto.edu/~dtarlow/">Daniel Tarlow</a>, and <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>
                  <br>
                  <em>10th European Conference on Computer Vision (ECCV 2008)</em>, 2008
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/articulated/RossTarlowZemel_ECCV2008.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/articulated/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/articulated/MeedsRossZemelRoweis_CVPR2008.pdf"><papertitle>Learning stick-figure models using nonparametric Bayesian priors over trees.</papertitle></a>
                  <br>
                  <a href="https://scholar.google.nl/citations?hl=en&pli=1&user=oxrYi1cAAAAJ">Edward Meeds</a>, David Ross, <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>, and <a href="http://www.cs.toronto.edu/~roweis/">Sam Roweis</a>
                  <br>
                  <em>IEEE Conference on Computer Vision and Pattern Recognition</em>, 2008
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/articulated/MeedsRossZemelRoweis_CVPR2008.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/articulated/RossTarlowZemel_WDV2007.pdf"><papertitle>Learning Articulated Skeletons From Motion.</papertitle></a>
                  <br>
                  David Ross, <a href="http://www.cs.toronto.edu/~dtarlow/">Daniel Tarlow</a>, and <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>
                  <br>
                  <em><a href="http://vision.jhu.edu/iccv2007-wdv/">Workshop on Dynamical Vision at ICCV</a></em>, 2007
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/articulated/RossTarlowZemel_WDV2007.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/articulated/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf"><papertitle>Incremental Learning for Robust Visual Tracking.</papertitle></a>
                  <br>
                  David Ross, <a href="http://vision.ucsd.edu/~jwlim/">Jongwoo Lim</a>, <a href="http://www.ifp.uiuc.edu/~rlin1/">Ruei-Sung Lin</a>, <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en/">Ming-Hsuan Yang</a>
                  <br>
                  <em>In the <a href="http://www.springerlink.com/">International Journal of Computer Vision, Special Issue: Learning for Vision</a></em>, 2008
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.ps.gz">PS.GZ</a> / <a href="https://www.cs.toronto.edu/~dross/ivt/RossLimLinYang_ijcv.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/ivt/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="http://www.journalofvision.org/7/8/15/Cohen-2007-jov-7-8-15.pdf"><papertitle>Inducing Features from Visual Noise.</papertitle></a>
                  <br>
                  <a href="http://people.umass.edu/alc/">Andrew Cohen</a>, <a href="http://mypage.iu.edu/~maplab/home1.html">Richard Shiffrin</a>, Jason Gold, David Ross, and Michael Ross
                  <br>
                  <em><a href="http://www.journalofvision.org/7/8/15/">Journal of Vision, 7(8):15</a></em>, 2007
                  <br>
                  <a href="http://www.journalofvision.org/7/8/15/Cohen-2007-jov-7-8-15.pdf">PDF</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/mcvq/RossZemel_JMLR06.pdf"><papertitle>Learning Parts-Based Representations of Data.</papertitle></a>
                  <br>
                  David Ross and <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>
                  <br>
                  <em><a href="http://jmlr.csail.mit.edu/papers/v7/">Journal of Machine Learning Research, 7(Nov):2369-2397</a></em>, 2006
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/mcvq/RossZemel_JMLR06.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/mcvq/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/cdf_icml06/RossOsinderoZemel_ICML06.pdf"><papertitle>Combining Discriminative Features to Infer Complex Trajectories.</papertitle></a>
                  <br>
                  David Ross, <a href="http://www.cs.toronto.edu/~osindero/">Simon Osindero</a>, and <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>
                  <br>
                  <em>In Proceedings of the Twenty-Third International Conference on Machine Learning</em>, 2006
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/cdf_icml06/RossOsinderoZemel_ICML06.ps.gz">PS.GZ</a> / <a href="https://www.cs.toronto.edu/~dross/cdf_icml06/RossOsinderoZemel_ICML06.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/cdf_icml06/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/ivt/LimRossLinYang_nips04.pdf"><papertitle>Incremental Learning for Visual Tracking.</papertitle></a>
                  <br>
                  <a href="http://vision.ucsd.edu/~jwlim/">Jongwoo Lim</a>, David Ross, <a href="http://www.ifp.uiuc.edu/~rlin1/">Ruei-Sung Lin</a>, <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en/">Ming-Hsuan Yang</a>
                  <br>
                  <em>In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, MIT Press</em>, 2005
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/ivt/LimRossLinYang_nips04.ps.gz">PS.GZ</a> / <a href="https://www.cs.toronto.edu/~dross/ivt/LimRossLinYang_nips04.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/ivt/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/ivt/LinRossLimYang_nips04.pdf"><papertitle>Adaptive Discriminative Generative Model and Its Applications.</papertitle></a>
                  <br>
                  <a href="http.ifp.uiuc.edu/~rlin1/">Ruei-Sung Lin</a>, David Ross, <a href="http://vision.ucsd.edu/~jwlim/">Jongwoo Lim</a>, <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en/">Ming-Hsuan Yang</a>
                  <br>
                  <em>In L. Saul, Y. Weiss, and L. Bottou, editors, Advances in Neural Information Processing Systems 17, MIT Press</em>, 2005
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/ivt/LinRossLimYang_nips04.ps.gz">PS.GZ</a> / <a href="https://www.cs.toronto.edu/~dross/ivt/LinRossLimYang_nips04.pdf">PDF</a> / <a href="http://www.ifp.uiuc.edu/~rlin1/adgm.html">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/ivt/RossLimYang_IVT.pdf"><papertitle>Adaptive Probabilistic Visual Tracking with Incremental Subspace Update.</papertitle></a>
                  <br>
                  David Ross, <a href="http://vision.ucsd.edu/~jwlim/">Jongwoo Lim</a>, <a href="https://scholar.google.com/citations?user=p9-ohHsAAAAJ&hl=en/">Ming-Hsuan Yang</a>
                  <br>
                  <em>In T. Pajdla and J. Matas, editors, Proc. Eighth European Conference on Computer Vision (ECCV 2004)</em>, 2004
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/ivt/RossLimYang_IVT.ps.gz">PS.GZ</a> / <a href="https://www.cs.toronto.edu/~dross/ivt/RossLimYang_IVT.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/ivt/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/mcvq/RossZemel_MCVQ.pdf"><papertitle>Multiple Cause Vector Quantization.</papertitle></a>
                  <br>
                  David Ross and <a href="http://www.cs.toronto.edu/~zemel/">Richard Zemel</a>
                  <br>
                  <em>In S. Becker, S. Thrun, and K. Obermayer, editors, Advances in Neural Information Processing Systems 15, MIT Press</em>, 2003
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/mcvq/RossZemel_MCVQ.ps.gz">PS.GZ</a> / <a href="https://www.cs.toronto.edu/~dross/mcvq/RossZemel_MCVQ.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/mcvq/">project website</a>
                  <p></p>
                </td>
            </tr>
            <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                  <a href="https://www.cs.toronto.edu/~dross/mcvq/Ross_msc_thesis.pdf"><papertitle>Learning Parts-Based Representations of Data (thesis version).</papertitle></a>
                  <br>
                  David Ross
                  <br>
                  <em>University of Toronto, M.Sc. Thesis</em>, 2003
                  <br>
                  <a href="https://www.cs.toronto.edu/~dross/mcvq/Ross_msc_thesis.ps.gz">PS.GZ</a> / <a href="https://www.cs.toronto.edu/~dross/mcvq/Ross_msc_thesis.pdf">PDF</a> / <a href="https://www.cs.toronto.edu/~dross/mcvq/">project website</a>
                  <p></p>
                </td>
            </tr>
             <tr>
                <td width="100%" valign="middle" style="padding:20px;">
                    BibTeX entries for all of the above are available <a href="https://www.cs.toronto.edu/~dross/Ross.bib">here</a>.
                </td>
            </tr>
        </tbody></table>

        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
          <tr>
            <td>
              <heading>Code</heading>
            </td>
          </tr>
          <tr>
            <td width="100%" valign="middle">
                D3D: Distilled 3D Networks TensorFlow code and pre-trained model checkpoints can be found <a href="https://github.com/princeton-vl/d3dhelper">here</a>.
                <br><br>
                AVA Atomic Visual Actions evaluation code can be found in the <a href="https://github.com/activitynet/ActivityNet/tree/master/Evaluation">ActivityNet GitHub repo</a>. Find the data <a href="http://research.google.com/ava/">here</a>.
                <br><br>
                The source code for most of my older research projects is <a href="https://www.cs.toronto.edu/~dross/code/">available for download here</a>. Included are <a href="https://www.cs.toronto.edu/~dross/code/#matlab">MATLAB implementations</a> of a number of machine learning & computer vision algorithms, but there are also a few <a href="https://www.cs.toronto.edu/~dross/code/#hacks">other hacks</a>.
                <br><br>
                Parallel Computing: Here is some code I've written/modified, as well as some getting-started tips for <a href="https://www.cs.toronto.edu/~dross/code/parallel.shtml">parallel computing using MATLAB</a>.
                <br><br>
                The code for the "Combining Discriminative Features" learning/tracking algorithm is available. <a href="https://www.cs.toronto.edu/~dross/code/cdf_2007-07-13.zip">cdf_2007-07-13.zip</a>
            </td>
          </tr>
        </tbody></table>
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr>
            <td style="padding:0px">
              <br>
              <p style="text-align:right;font-size:small;">
                 Last Updated: May 2025.
              </p>
            </td>
          </tr>
        </tbody></table>
      </td>
    </tr>
  </table>
</body>

</html>
